{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:03:06.813554Z",
     "start_time": "2024-12-19T04:03:01.439708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_preprocess(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset and preprocess the text and labels.\n",
    "    Args:\n",
    "        data_path (str): Path to the dataset CSV file.\n",
    "    Returns:\n",
    "        list: Preprocessed sentences as lists of tokens.\n",
    "        list: Corresponding author labels.\n",
    "        dict: Word-to-index mapping.\n",
    "        dict: Index-to-word mapping.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    data['text'] = data['text'].str.lower()\n",
    "\n",
    "    # Tokenize text into words\n",
    "    tokenized_sentences = [sentence.split() for sentence in data['text']]\n",
    "\n",
    "    # Create vocabulary and mappings\n",
    "    vocab = set(word for sentence in tokenized_sentences for word in sentence)\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "    # Map sentences to sequences of indices\n",
    "    tokenized_sentences = [[word_to_index[word] for word in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "    return tokenized_sentences, data['author'].tolist(), word_to_index, index_to_word\n",
    "\n",
    "# Load data\n",
    "data_path = \"../data/Russian/author_data.csv\"  # Replace with your file path\n",
    "sentences, authors, word_to_index, index_to_word = load_and_preprocess(data_path)\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n"
   ],
   "id": "281f6ff41d8da5c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 363992\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:03:06.828152Z",
     "start_time": "2024-12-19T04:03:06.814984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_by_author(sentences, authors):\n",
    "    \"\"\"\n",
    "    Group sentences by author for separate HMM training.\n",
    "    Args:\n",
    "        sentences (list): List of tokenized sentences.\n",
    "        authors (list): List of corresponding authors.\n",
    "    Returns:\n",
    "        dict: Dictionary where keys are authors and values are lists of tokenized sentences.\n",
    "    \"\"\"\n",
    "    author_data = defaultdict(list)\n",
    "    for sentence, author in zip(sentences, authors):\n",
    "        author_data[author].append(sentence)\n",
    "    return author_data\n",
    "\n",
    "# Group data by author\n",
    "author_data = group_by_author(sentences, authors)\n",
    "print(f\"Authors: {list(author_data.keys())}\")\n"
   ],
   "id": "2c8c1ba69df3fa5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: ['bulgakov ', 'chekhov ', 'dostoevskiy ', 'gorky ', 'tolstoy ']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:14:13.070045Z",
     "start_time": "2024-12-19T04:11:24.871770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_hmm_for_author(sentences, n_states=5):\n",
    "    \"\"\"\n",
    "    Train an HMM on tokenized sentences for a single author.\n",
    "    Args:\n",
    "        sentences (list): List of tokenized sentences (as sequences of word indices).\n",
    "        n_states (int): Number of hidden states in the HMM.\n",
    "    Returns:\n",
    "        model: Trained HMM model.\n",
    "    \"\"\"\n",
    "    # Flatten all sentences into a single observation sequence\n",
    "    observations = np.concatenate(sentences).reshape(-1, 1)\n",
    "\n",
    "    # Initialize and train the HMM\n",
    "    model = hmm.MultinomialHMM(n_components=n_states, random_state=42, n_iter=100)\n",
    "    model.fit(observations)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train HMMs for each author\n",
    "hmm_models = {}\n",
    "for author, sentences in author_data.items():\n",
    "    print(f\"Training HMM for {author}...\")\n",
    "    hmm_models[author] = train_hmm_for_author(sentences, n_states=20)\n",
    "print(\"HMM training completed for all authors.\")"
   ],
   "id": "317e473d17d6b57c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for bulgakov ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for chekhov ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for dostoevskiy ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for gorky ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for tolstoy ...\n",
      "HMM training completed for all authors.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:06:18.749575Z",
     "start_time": "2024-12-19T04:06:18.734563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(model, index_to_word, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate text using a trained HMM.\n",
    "    Args:\n",
    "        model: Trained HMM model.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "        max_len (int): Maximum length of the generated text.\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    model.n_trials = 1  # Ensure n_trials is set for sampling\n",
    "    random_state = np.random.RandomState()  # Reinitialize random state for variability\n",
    "    observations, _ = model.sample(n_samples=max_len, random_state=random_state)\n",
    "    generated_words = [index_to_word[idx[0]] for idx in observations]\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "\n",
    "# Example usage: Generate text for an author\n",
    "target_author = \"tolstoy \"  # Replace with an author in your dataset\n",
    "if target_author in hmm_models:\n",
    "    generated_text = generate_text(hmm_models[target_author], index_to_word, max_len=50)\n",
    "    print(f\"Generated text for {target_author}:\\n{generated_text}\")\n",
    "else:\n",
    "    print(f\"No HMM model found for author: {target_author}\")\n"
   ],
   "id": "5f6aed6428ce949a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for tolstoy :\n",
      "купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую купеческую\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:10:36.901213Z",
     "start_time": "2024-12-19T04:10:36.894298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_with_temperature(model, index_to_word, max_len=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using a trained HMM with temperature scaling for diversification.\n",
    "    Args:\n",
    "        model: Trained HMM model.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "        max_len (int): Maximum length of the generated text.\n",
    "        temperature (float): Controls randomness; higher values increase diversity.\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    model.n_trials = 1  # Ensure n_trials is set for sampling\n",
    "    random_state = np.random.RandomState()  # Add randomness for variability\n",
    "\n",
    "    # Generate the initial state sequence\n",
    "    states = [random_state.choice(model.startprob_.size, p=model.startprob_)]\n",
    "    words = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Scale the emission probabilities with temperature\n",
    "        emission_probs = model.emissionprob_[states[-1]]\n",
    "        scaled_probs = emission_probs ** (1 / temperature)\n",
    "        scaled_probs /= scaled_probs.sum()  # Normalize\n",
    "\n",
    "        # Sample the next word and state\n",
    "        next_word_idx = random_state.choice(len(scaled_probs), p=scaled_probs)\n",
    "        next_state = random_state.choice(model.transmat_.shape[1], p=model.transmat_[states[-1]])\n",
    "        words.append(index_to_word[next_word_idx])\n",
    "        states.append(next_state)\n",
    "\n",
    "    return \" \".join(words)\n"
   ],
   "id": "836ec374bdf3d819",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:10:38.257323Z",
     "start_time": "2024-12-19T04:10:38.250609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: Generate text with temperature scaling\n",
    "target_author = \"chekhov \"  # Replace with an author in your dataset\n",
    "if target_author in hmm_models:\n",
    "    generated_text = generate_text_with_temperature(hmm_models[target_author], index_to_word, max_len=50, temperature=0.5)\n",
    "    print(f\"Generated text for {target_author} with temperature scaling:\\n{generated_text}\")\n",
    "else:\n",
    "    print(f\"No HMM model found for author: {target_author}\")"
   ],
   "id": "3a8c8e995561528d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for chekhov  with temperature scaling:\n",
      "трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке,\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:10:03.556146Z",
     "start_time": "2024-12-19T04:10:03.550234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_with_penalty(model, index_to_word, max_len=50, penalty=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using a trained HMM with a penalty for repeated words.\n",
    "    Args:\n",
    "        model: Trained HMM model.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "        max_len (int): Maximum length of the generated text.\n",
    "        penalty (float): Probability multiplier for already-generated words.\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    model.n_trials = 1  # Ensure n_trials is set for sampling\n",
    "    random_state = np.random.RandomState()  # Add randomness for variability\n",
    "\n",
    "    states = [random_state.choice(model.startprob_.size, p=model.startprob_)]\n",
    "    words = []\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        emission_probs = model.emissionprob_[states[-1]]\n",
    "\n",
    "        # Apply penalty to already-generated words\n",
    "        for idx, prob in enumerate(emission_probs):\n",
    "            if index_to_word[idx] in words:\n",
    "                emission_probs[idx] *= penalty\n",
    "\n",
    "        emission_probs /= emission_probs.sum()  # Normalize\n",
    "\n",
    "        # Sample the next word and state\n",
    "        next_word_idx = random_state.choice(len(emission_probs), p=emission_probs)\n",
    "        next_state = random_state.choice(model.transmat_.shape[1], p=model.transmat_[states[-1]])\n",
    "\n",
    "        generated_word = index_to_word[next_word_idx]\n",
    "        words.append(generated_word)\n",
    "        word_counts[generated_word] += 1\n",
    "        states.append(next_state)\n",
    "\n",
    "    return \" \".join(words)\n"
   ],
   "id": "717c0db429ee23b6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:10:10.486423Z",
     "start_time": "2024-12-19T04:10:10.477943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: Generate text with penalty for repeated words\n",
    "target_author = \"tolstoy \"  # Replace with an author in your\n",
    "if target_author in hmm_models:\n",
    "    generated_text = generate_text_with_penalty(hmm_models[target_author], index_to_word, max_len=50, penalty=0.5)\n",
    "    print(f\"Generated text for {target_author} with penalty for repeated words:\\n{generated_text}\")\n",
    "else:\n",
    "    print(f\"No HMM model found for author: {target_author}\")"
   ],
   "id": "3ae45bda74958e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for tolstoy  with penalty for repeated words:\n",
      "трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке,\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:14:13.081109Z",
     "start_time": "2024-12-19T04:14:13.072051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_with_randomness(model, index_to_word, max_len=50, randomness=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using a trained HMM with enhanced randomness for variability.\n",
    "    Args:\n",
    "        model: Trained HMM model.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "        max_len (int): Maximum length of the generated text.\n",
    "        randomness (float): Adds noise to probabilities for more diversity (higher = more random).\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    model.n_trials = 1\n",
    "    random_state = np.random.RandomState()\n",
    "\n",
    "    states = [random_state.choice(model.startprob_.size, p=model.startprob_)]\n",
    "    words = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Get emission probabilities\n",
    "        emission_probs = model.emissionprob_[states[-1]]\n",
    "\n",
    "        # Add randomness by perturbing probabilities\n",
    "        noisy_probs = emission_probs + randomness * random_state.rand(len(emission_probs))\n",
    "        noisy_probs /= noisy_probs.sum()\n",
    "\n",
    "        # Sample the next word\n",
    "        next_word_idx = random_state.choice(len(noisy_probs), p=noisy_probs)\n",
    "\n",
    "        # Sample the next state\n",
    "        trans_probs = model.transmat_[states[-1]]\n",
    "        next_state = random_state.choice(model.transmat_.shape[1], p=trans_probs)\n",
    "\n",
    "        words.append(index_to_word[next_word_idx])\n",
    "        states.append(next_state)\n",
    "\n",
    "    return \" \".join(words)\n"
   ],
   "id": "30e7494eea7768a5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:14:59.344054Z",
     "start_time": "2024-12-19T04:14:59.335206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: Generate text with enhanced randomness\n",
    "target_author = \"dostoevskiy \"  # Replace with an author in your\n",
    "if target_author in hmm_models:\n",
    "    generated_text = generate_text_with_randomness(hmm_models[target_author], index_to_word, max_len=50, randomness=0.9)\n",
    "    print(f\"Generated text for {target_author} with enhanced randomness:\\n{generated_text}\")"
   ],
   "id": "5b8d7cdd13648349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for dostoevskiy  with enhanced randomness:\n",
      "трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке, трейчке,\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:15:25.909913Z",
     "start_time": "2024-12-19T04:15:25.894933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Observe the transition matrix for an author\n",
    "target_author = \"tolstoy \"  # Replace with an author in your\n",
    "if target_author in hmm_models:\n",
    "    print(f\"Transition matrix for {target_author}:\\n{hmm_models[target_author].transmat_}\")"
   ],
   "id": "11d23a868c07ec7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix for tolstoy :\n",
      "[[5.81401480e-19 5.79198957e-30 1.96413502e-12 8.12086841e-11\n",
      "  6.16172878e-06 6.50649432e-01 3.10801917e-01 3.69086827e-05\n",
      "  9.39046886e-22 1.38857356e-27 6.70410949e-09 2.52870227e-02\n",
      "  1.01545247e-11 1.03340578e-17 3.04498906e-23 6.16108630e-03\n",
      "  7.52783445e-46 1.05000961e-03 6.00745421e-03 1.32895600e-09]\n",
      " [8.12634909e-02 3.82403768e-10 1.10497562e-10 2.82103663e-03\n",
      "  1.40991873e-01 5.53151843e-19 6.51445437e-03 8.49877784e-03\n",
      "  3.58289589e-06 1.95817363e-32 1.36552999e-30 1.37111292e-10\n",
      "  2.21925677e-01 2.83415943e-08 2.38610431e-13 2.68627898e-11\n",
      "  3.59313349e-01 1.66821246e-04 1.95081574e-02 1.58992750e-01]\n",
      " [6.16571079e-03 4.97065466e-11 6.36799651e-14 7.99832669e-03\n",
      "  3.09125356e-44 1.14565575e-08 1.66590675e-19 1.37144496e-01\n",
      "  8.86377268e-07 7.26040583e-10 2.14854587e-01 3.79529915e-07\n",
      "  5.49301637e-12 2.22929345e-05 7.59165292e-27 6.48688693e-02\n",
      "  7.39327044e-18 5.68787073e-01 1.57366965e-04 1.46580899e-13]\n",
      " [4.62398085e-09 2.44635573e-04 2.92382121e-21 3.00142360e-10\n",
      "  3.63989973e-28 9.37974377e-04 3.46862738e-06 3.52128141e-04\n",
      "  1.38662220e-03 1.02319255e-09 4.71126008e-01 3.83361587e-12\n",
      "  3.98091579e-02 6.80720593e-06 5.40047194e-21 2.76734808e-01\n",
      "  9.07689082e-10 3.72719410e-03 2.05351808e-01 3.19380633e-04]\n",
      " [1.36336897e-16 4.16718840e-05 1.23346925e-20 1.12194550e-46\n",
      "  5.64257217e-06 1.77287801e-04 1.03759775e-03 1.63149582e-10\n",
      "  1.65045338e-04 2.10692671e-04 2.49284662e-21 2.72766476e-12\n",
      "  6.25327810e-01 9.37547889e-02 9.32231447e-03 1.53572098e-05\n",
      "  5.96694386e-15 8.53748936e-12 1.45142697e-04 2.69796649e-01]\n",
      " [1.37087457e-01 4.92211659e-37 3.48718474e-08 4.05983151e-01\n",
      "  1.95012181e-11 3.23481391e-02 3.11089678e-16 2.17178274e-01\n",
      "  1.06731459e-05 4.86931229e-05 6.88073572e-18 5.93675695e-02\n",
      "  5.95271335e-04 1.05591107e-09 1.18225290e-02 4.68780652e-02\n",
      "  1.21366643e-06 8.97704451e-03 6.85797112e-04 7.90160862e-02]\n",
      " [4.77966109e-08 2.67944738e-04 3.51157645e-06 2.13315667e-10\n",
      "  7.43614128e-30 3.08361174e-01 1.84285741e-17 8.22283256e-02\n",
      "  1.18444112e-03 2.83151613e-25 6.97211881e-05 2.54158555e-02\n",
      "  2.77541562e-05 1.56551164e-01 1.08144539e-06 1.84116902e-31\n",
      "  4.25793149e-01 2.62268695e-07 1.18051528e-15 9.55678784e-05]\n",
      " [3.36496331e-04 5.42812883e-01 1.02152496e-05 3.89238087e-08\n",
      "  1.45156931e-09 1.98073250e-37 2.44996535e-27 5.99107180e-02\n",
      "  4.49010044e-07 9.24745907e-07 8.02705013e-16 1.38731204e-08\n",
      "  1.54974366e-04 4.02858378e-09 1.46965963e-06 3.21494642e-04\n",
      "  1.27571194e-23 4.00062997e-32 3.96450308e-01 8.20242272e-09]\n",
      " [6.05198012e-08 1.08611310e-01 1.69560138e-01 2.48687355e-15\n",
      "  4.23748385e-21 1.15660510e-21 4.05516181e-24 5.98796005e-03\n",
      "  1.02225148e-19 3.40421416e-05 7.69802888e-04 3.67993548e-12\n",
      "  1.16986335e-03 6.95560131e-01 9.35565736e-10 1.62134405e-10\n",
      "  1.71283860e-02 1.17787891e-03 6.71393587e-21 4.26540684e-07]\n",
      " [3.26238532e-10 1.63186245e-08 3.11557921e-10 9.31633102e-01\n",
      "  2.63811337e-04 5.65962779e-11 9.18911311e-04 2.39373029e-02\n",
      "  4.07068459e-21 4.08712612e-25 2.03424689e-07 2.05697217e-09\n",
      "  3.27646060e-17 1.70617376e-04 8.05030395e-22 4.44019602e-23\n",
      "  2.44491110e-03 9.57124899e-22 7.46291101e-09 4.06311136e-02]\n",
      " [5.47382195e-01 1.34978245e-09 2.69509996e-03 1.48722354e-08\n",
      "  3.48667450e-20 5.28711015e-40 4.27745620e-26 1.05489604e-19\n",
      "  1.19072479e-03 2.00261303e-01 5.48383740e-12 4.07180197e-14\n",
      "  2.06650373e-39 2.09953598e-28 1.17023332e-06 9.57712673e-24\n",
      "  2.48420726e-01 3.97217431e-05 5.91527337e-08 8.98344261e-06]\n",
      " [3.60576360e-28 9.23124636e-02 3.79985909e-08 1.85588557e-12\n",
      "  5.39167014e-08 5.37219821e-06 1.82876367e-01 1.93255498e-04\n",
      "  1.66341011e-11 1.31042831e-04 3.95079531e-02 9.11284087e-02\n",
      "  1.45245231e-05 6.34039129e-02 3.26112153e-28 8.34575660e-10\n",
      "  3.67799156e-01 7.66745748e-06 1.55115693e-01 7.50408960e-03]\n",
      " [4.31252945e-08 1.99230750e-25 3.00816743e-02 9.90680009e-03\n",
      "  7.28321803e-02 2.25058577e-07 7.42267934e-01 2.88868082e-13\n",
      "  1.25637841e-04 2.40919670e-19 4.00639557e-06 3.98424702e-06\n",
      "  1.30275954e-05 1.36178702e-01 6.61655263e-18 6.88758064e-03\n",
      "  1.69818910e-03 9.37604203e-18 1.49100383e-09 1.40823758e-08]\n",
      " [4.46383827e-01 5.54497961e-06 3.09468310e-08 1.73230607e-01\n",
      "  7.50762587e-17 2.32364658e-06 5.80279875e-07 2.37149594e-06\n",
      "  3.61732967e-04 1.33491317e-22 4.70028935e-18 2.36758060e-17\n",
      "  5.25433956e-15 3.78978812e-01 1.03416989e-03 1.59920766e-14\n",
      "  1.20008266e-15 3.05347943e-15 1.42673884e-18 6.54966806e-14]\n",
      " [1.33590218e-05 9.50049252e-28 1.10924502e-03 8.18940618e-01\n",
      "  6.38120775e-24 1.64468263e-01 2.55934800e-14 2.89949690e-08\n",
      "  8.13882998e-04 2.45526932e-06 2.37989281e-28 1.42318287e-02\n",
      "  1.84985574e-05 4.88472442e-19 4.12588641e-05 4.89900982e-11\n",
      "  1.37618953e-33 3.90197391e-13 4.50404583e-18 3.60561923e-04]\n",
      " [1.02517793e-02 7.07615618e-22 2.74021718e-05 5.89116607e-08\n",
      "  1.33932088e-11 1.86600075e-04 1.25526055e-09 5.17128655e-05\n",
      "  1.66277777e-20 7.66626337e-13 3.04839998e-15 7.03126353e-16\n",
      "  1.42730115e-22 2.14513266e-08 1.13436067e-19 6.99026905e-01\n",
      "  2.05391082e-02 5.26725855e-16 2.69900215e-01 1.61961390e-05]\n",
      " [1.56218181e-01 4.69824520e-09 3.80407197e-05 4.69096696e-18\n",
      "  4.74771781e-10 1.73975081e-15 9.94236637e-05 1.02809084e-08\n",
      "  3.22102623e-23 4.32808674e-09 3.87430411e-11 1.22606779e-01\n",
      "  8.72320786e-06 2.10314012e-08 7.20576922e-01 2.28490113e-17\n",
      "  8.42527754e-23 4.51890290e-04 2.38817340e-16 3.63602127e-13]\n",
      " [8.04818308e-16 1.13178365e-03 4.92653435e-09 2.95347485e-08\n",
      "  2.79713959e-07 7.94996852e-07 1.17017208e-02 8.17474240e-13\n",
      "  1.90072162e-08 5.51878954e-01 8.93098865e-19 3.54738971e-04\n",
      "  2.96507492e-17 2.77720475e-04 4.17312670e-01 1.63315754e-16\n",
      "  3.16886070e-12 1.27073986e-29 1.73412827e-02 1.89388222e-09]\n",
      " [1.45921390e-19 2.48860302e-18 2.08405479e-02 3.11002648e-13\n",
      "  2.71403236e-03 1.29738379e-03 2.03051587e-12 2.94260531e-15\n",
      "  4.03395147e-05 3.79728295e-07 1.00064359e-16 2.33143627e-06\n",
      "  2.78600253e-35 4.74181779e-01 1.40225642e-03 1.89602419e-03\n",
      "  3.15733957e-05 6.86306220e-08 4.97593050e-01 2.32788246e-07]\n",
      " [8.39062677e-01 5.68743804e-19 4.26125677e-02 1.88792244e-05\n",
      "  4.07114099e-26 2.00485332e-10 2.79299131e-06 4.26963546e-10\n",
      "  1.56834862e-03 7.38871585e-14 6.54270529e-18 6.18281746e-07\n",
      "  1.16720286e-01 1.31085229e-05 1.52032703e-38 7.08504089e-16\n",
      "  1.92410761e-17 2.91221975e-22 7.20502148e-07 2.35283977e-24]]\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
