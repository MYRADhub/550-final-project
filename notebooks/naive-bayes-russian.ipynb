{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:27:22.975323Z",
     "start_time": "2024-12-19T00:27:20.789153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess(data_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset for Naive Bayes.\n",
    "    Args:\n",
    "        data_path (str): Path to the dataset CSV file.\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset with 'text' and 'author' columns.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    return data\n",
    "\n",
    "# Prepare data\n",
    "data_path = \"../data/Russian/all_tokenized_data.csv\"  # Replace with your dataset\n",
    "data = load_and_preprocess(data_path)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], data['author'], test_size=0.2, random_state=42\n",
    ")"
   ],
   "id": "9995b9680b3d3742",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:30:49.165666Z",
     "start_time": "2024-12-19T00:27:22.976794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def text_to_pos_features(text):\n",
    "    tokens = text.split()\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Generate POS tags\n",
    "    return \" \".join([f\"{word}_{tag}\" for word, tag in pos_tags])\n",
    "\n",
    "# Apply POS tagging to the dataset\n",
    "data['text_pos'] = data['text'].apply(text_to_pos_features)\n",
    "\n",
    "# Use POS-tagged text in the vectorizer\n",
    "X_train_vec = vectorizer.fit_transform(data['text_pos'])\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = nb_model.score(X_test_vec, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "ac530ab160598226",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 4]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Train Naive Bayes model\u001B[39;00m\n\u001B[0;32m     20\u001B[0m nb_model \u001B[38;5;241m=\u001B[39m MultinomialNB(alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m nb_model\u001B[38;5;241m.\u001B[39mfit(X_train_vec, y_train)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m     24\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m nb_model\u001B[38;5;241m.\u001B[39mscore(X_test_vec, y_test)\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:735\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    715\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    716\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[0;32m    717\u001B[0m \n\u001B[0;32m    718\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    733\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 735\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X_y(X, y)\n\u001B[0;32m    736\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    738\u001B[0m     labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:581\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[1;34m(self, X, y, reset)\u001B[0m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    580\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m validate_data(\u001B[38;5;28mself\u001B[39m, X, y, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, reset\u001B[38;5;241m=\u001B[39mreset)\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001B[0m, in \u001B[0;36mvalidate_data\u001B[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[0m\n\u001B[0;32m   2959\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m   2960\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2961\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m   2962\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m   2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1389\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1370\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1371\u001B[0m     X,\n\u001B[0;32m   1372\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1384\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1385\u001B[0m )\n\u001B[0;32m   1387\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1389\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32mE:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    473\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 475\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    476\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    477\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    478\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [5, 4]"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:32:43.899923Z",
     "start_time": "2024-12-19T00:32:37.782536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(nb_model, vectorizer, author, max_len=100, seed_word=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text in the style of a given author using a trained Naive Bayes model with sampling.\n",
    "    Args:\n",
    "        nb_model: Trained Naive Bayes model.\n",
    "        vectorizer: Fitted CountVectorizer.\n",
    "        author (str): Author whose style to mimic.\n",
    "        max_len (int): Maximum length of the generated text.\n",
    "        seed_word (str): Optional starting word.\n",
    "        temperature (float): Sampling temperature to control randomness.\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    if seed_word is None:\n",
    "        # Randomly choose a starting word from the vocabulary\n",
    "        seed_word = random.choice(vectorizer.get_feature_names_out())\n",
    "\n",
    "    generated_text = [seed_word]\n",
    "    current_word = seed_word\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        # Create a pseudo-document for the current word\n",
    "        pseudo_doc = \" \".join(generated_text)\n",
    "\n",
    "        # Vectorize the pseudo-document\n",
    "        vec = vectorizer.transform([pseudo_doc])\n",
    "\n",
    "        # Get probabilities for the next word\n",
    "        author_index = np.where(nb_model.classes_ == author)[0][0]\n",
    "        word_probs = np.exp(nb_model.feature_log_prob_[author_index])  # Convert log probs to probabilities\n",
    "\n",
    "        # Adjust probabilities with temperature\n",
    "        word_probs = word_probs ** (1 / temperature)\n",
    "        word_probs /= np.sum(word_probs)  # Normalize probabilities\n",
    "\n",
    "        # Sample the next word based on probabilities\n",
    "        next_word_idx = np.random.choice(len(word_probs), p=word_probs)\n",
    "        next_word = vectorizer.get_feature_names_out()[next_word_idx]\n",
    "\n",
    "        generated_text.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "# Example usage\n",
    "target_author = \"tolstoy \"  # Replace with an actual author name from your dataset\n",
    "seed_word = \"даже\"  # Optional seed word\n",
    "generated_text = generate_text(nb_model, vectorizer, target_author, max_len=100, temperature=0.7)\n",
    "print(f\"Generated text in the style of {target_author}:\\n{generated_text}\")\n"
   ],
   "id": "ebc53214a5a2fc2",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'classes_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 48\u001B[0m\n\u001B[0;32m     46\u001B[0m target_author \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtolstoy \u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with an actual author name from your dataset\u001B[39;00m\n\u001B[0;32m     47\u001B[0m seed_word \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mдаже\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Optional seed word\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m generate_text(nb_model, vectorizer, target_author, max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated text in the style of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_author\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mgenerated_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[10], line 29\u001B[0m, in \u001B[0;36mgenerate_text\u001B[1;34m(nb_model, vectorizer, author, max_len, seed_word, temperature)\u001B[0m\n\u001B[0;32m     26\u001B[0m vec \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mtransform([pseudo_doc])\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Get probabilities for the next word\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m author_index \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(nb_model\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m==\u001B[39m author)[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     30\u001B[0m word_probs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(nb_model\u001B[38;5;241m.\u001B[39mfeature_log_prob_[author_index])  \u001B[38;5;66;03m# Convert log probs to probabilities\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Adjust probabilities with temperature\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'MultinomialNB' object has no attribute 'classes_'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:07:28.833320Z",
     "start_time": "2024-12-19T00:07:28.826073Z"
    }
   },
   "cell_type": "code",
   "source": "nb_model.classes_",
   "id": "9a6790ada946d87b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bulgakov ', 'dostoevskiy ', 'gorky ', 'tolstoy '], dtype='<U12')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:07:05.700509Z",
     "start_time": "2024-12-19T00:07:05.690286Z"
    }
   },
   "cell_type": "code",
   "source": "len(nb_model.feature_log_prob_[0])",
   "id": "3800a66aed46d2f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189607"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c60f56d948e149c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "58799d087feafcb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aab397d05fb573d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T00:32:54.421202Z",
     "start_time": "2024-12-19T00:32:54.416953Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4872fc2b23f5e986",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
